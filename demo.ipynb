{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d677fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import getpass\n",
    "\n",
    "# nv-ingest imports\n",
    "from nv_ingest.framework.orchestration.ray.util.pipeline.pipeline_runners import run_pipeline\n",
    "from nv_ingest_client.client import Ingestor, NvIngestClient\n",
    "from nv_ingest_api.util.message_brokers.simple_message_broker import SimpleClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0b8fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] NVIDIA API Key detected in environment.\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "    print(\"[ACTION REQUIRED] Please enter your NVIDIA API Key (starts with 'nvapi-').\")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter NVIDIA API Key: \")\n",
    "else:\n",
    "    print(\"[INFO] NVIDIA API Key detected in environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dca5b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Ingestion Pipeline (Library Mode)...\n",
      "DEBUG: Direct print to stdout - should appear in parent process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Direct write to stderr - should appear in parent process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Submitting job...\n",
      "[SUCCESS] Extracted 452 items from document.\n"
     ]
    }
   ],
   "source": [
    "# 1. Download Sample PDF (World Bank Peru 2017)\n",
    "# PDF_URL = \"https://documents1.worldbank.org/curated/en/484531533637705178/pdf/129273-WP-PUBLIC-Peru-2017.pdf\"\n",
    "# PDF_PATH = \"129273-WP-PUBLIC-Peru-2017.pdf\"\n",
    "PDF_PATH = \"./pdf/ระบบซื้อ.pdf\"\n",
    "\n",
    "# if not Path(PDF_PATH).exists():\n",
    "#     print(f\"[INFO] Downloading {PDF_PATH}...\")\n",
    "#     response = requests.get(PDF_URL)\n",
    "#     with open(PDF_PATH, \"wb\") as f:\n",
    "#         f.write(response.content)\n",
    "\n",
    "# 2. Start nv-ingest in Library Mode\n",
    "# This spins up the pipeline locally but uses remote NIMs by default for inference.\n",
    "print(\"[INFO] Starting Ingestion Pipeline (Library Mode)...\")\n",
    "run_pipeline(\n",
    "    block=False,\n",
    "    disable_dynamic_scaling=True,\n",
    "    run_in_subprocess=True,\n",
    "    quiet=True\n",
    ")\n",
    "time.sleep(15) # Warmup\n",
    "\n",
    "client = NvIngestClient(\n",
    "    message_client_allocator=SimpleClient,\n",
    "    message_client_port=7671, # Default LibMode port\n",
    "    message_client_hostname=\"localhost\"\n",
    ")\n",
    "\n",
    "print(\"[INFO] Submitting job...\")\n",
    "\n",
    "# 4. Extract Content\n",
    "ingestor = (\n",
    "    Ingestor(client=client)\n",
    "    .files([PDF_PATH])\n",
    "    .extract(\n",
    "        extract_text=True,\n",
    "        extract_tables=True,\n",
    "        extract_charts=True,  # Triggers remote YOLOX for chart crops\n",
    "        extract_images=True, # Focus on charts/tables for VLM\n",
    "        extract_method=\"pdfium\", # CPU-based text extraction\n",
    "        table_output_format=\"markdown\"\n",
    "    )\n",
    ")\n",
    "\n",
    "job_results = ingestor.ingest()\n",
    "extracted_data = job_results[0]\n",
    "print(f\"[SUCCESS] Extracted {len(extracted_data)} items from document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabb1266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTED TABLE ITEM\n",
      "============================================================\n",
      "Type: table\n",
      "Page: 109\n",
      "Has Image Content? False\n",
      "\n",
      "Markdown Content (Snippet):\n",
      "| Todey Zoom In Toom Out |  |  |  |  |  |  |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| 1 sumen 2558 |  |  |  |  |  |  |\n",
      "|  |  |  |  |  | and |  |\n",
      "|  |  |  |  | 3 |  |  |\n",
      "|  | 30 |  |  |  |  |  |\n",
      "|  |  |  |  | 10 |  |  |\n",
      "| 13 | 24 |  |  |  |  | 29 |\n",
      "|  | 21 | 22 | 23 | 24 | 25 |  |...\n"
     ]
    }
   ],
   "source": [
    "table_items = [item for item in extracted_data if item[\"metadata\"][\"content_metadata\"].get(\"subtype\") == \"table\"]\n",
    "\n",
    "if table_items:\n",
    "    # Let's inspect a table from the Appendix (usually near the end of the doc)\n",
    "    sample = table_items[-1][\"metadata\"]\n",
    "    print(\"=\"*60)\n",
    "    print(\"EXTRACTED TABLE ITEM\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Type: {sample['content_metadata']['subtype']}\")\n",
    "    print(f\"Page: {sample['content_metadata']['page_number']}\")\n",
    "\n",
    "    image_meta = sample.get(\"image_metadata\") or {}\n",
    "    has_image = bool(image_meta.get(\"content\"))\n",
    "\n",
    "    print(f\"Has Image Content? {has_image}\")\n",
    "\n",
    "    print(\"\\nMarkdown Content (Snippet):\")\n",
    "    table_meta = sample.get(\"table_metadata\") or {}\n",
    "    content = table_meta.get(\"table_content\", \"\")\n",
    "    print(content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2234ff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Transformers Version: 4.46.3\n",
      "[INFO] Loading Embedding Model: nvidia/llama-nemotron-embed-vl-1b-v2...\n",
      "[INFO] GPU Compute Capability: 8.9\n",
      "[INFO] Using bfloat16 precision (Ampere+ detected).\n",
      "[INFO] Flash Attention 2 enabled.\n",
      "[INFO] Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_image_token, max_input_tiles, pad_to_multiple_of, dynamic_image_size, p_max_length, use_thumbnail, passage_prefix, image_size, norm_type, template, query_prefix, num_channels, padding, system_message, q_max_length. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating Multimodal Embeddings...\n",
      "[SUCCESS] Indexed 452 items.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoProcessor, AutoConfig\n",
    "from pymilvus import MilvusClient\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "from PIL import Image\n",
    "import base64\n",
    "import transformers\n",
    "\n",
    "# Configuration\n",
    "HF_EMBED_MODEL_ID = \"nvidia/llama-nemotron-embed-vl-1b-v2\"\n",
    "COLLECTION_NAME = \"worldbank_peru_2017\"\n",
    "MILVUS_URI = \"milvus_wb_demo.db\"\n",
    "\n",
    "print(f\"[INFO] Transformers Version: {transformers.__version__}\")\n",
    "print(f\"[INFO] Loading Embedding Model: {HF_EMBED_MODEL_ID}...\")\n",
    "\n",
    "# --- 1. HARDWARE-AWARE CONFIGURATION PATCH ---\n",
    "# Check GPU capabilities to ensure T4 (Turing) stability and H100 (Hopper) performance.\n",
    "use_flash_attn = False\n",
    "model_dtype = torch.float32 # Default fallback\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    print(f\"[INFO] GPU Compute Capability: {major}.{minor}\")\n",
    "\n",
    "    # Precision: Use bfloat16 for Ampere+ (8.0+), float16 for Turing (7.5) or older\n",
    "    if major >= 8:\n",
    "        model_dtype = torch.bfloat16\n",
    "        print(\"[INFO] Using bfloat16 precision (Ampere+ detected).\")\n",
    "    else:\n",
    "        model_dtype = torch.float16\n",
    "        print(\"[INFO] Using float16 precision (Turing/Older detected).\")\n",
    "\n",
    "    # Attention: Flash Attention 2 requires Ampere+ (8.0+)\n",
    "    if major >= 8:\n",
    "        try:\n",
    "            import flash_attn\n",
    "            use_flash_attn = True\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    if use_flash_attn:\n",
    "        attn_impl = \"flash_attention_2\"\n",
    "        print(\"[INFO] Flash Attention 2 enabled.\")\n",
    "        config = None\n",
    "    else:\n",
    "        attn_impl = \"eager\"\n",
    "        reason = \"Hardware incompatible (needs Ampere+)\" if major < 8 else \"Library not installed\"\n",
    "        print(f\"[WARNING] Flash Attention 2 disabled: {reason}. Forcing 'eager' mode.\")\n",
    "\n",
    "        # --- CONFIG PATCH ---\n",
    "        config = AutoConfig.from_pretrained(HF_EMBED_MODEL_ID, trust_remote_code=True)\n",
    "        if hasattr(config, \"llm_config\"):\n",
    "            original_config_class = config.llm_config.__class__\n",
    "            class SafeConfig(original_config_class):\n",
    "                def __setattr__(self, name, value):\n",
    "                    if name == \"_attn_implementation\" and value == \"flash_attention_2\":\n",
    "                        return\n",
    "                    super().__setattr__(name, value)\n",
    "            config.llm_config.__class__ = SafeConfig\n",
    "            config.llm_config._attn_implementation = \"eager\"\n",
    "else:\n",
    "    attn_impl = \"eager\"\n",
    "    config = None\n",
    "    print(\"[WARNING] Running on CPU.\")\n",
    "\n",
    "# --- 2. WORKAROUND FOR TOKENIZER BACKEND BUG ---\n",
    "# Even with downgraded transformers, sometimes the backend class needs patching \n",
    "# for specific model configurations.\n",
    "try:\n",
    "    from transformers.tokenization_utils_base import TokenizersBackend as OriginalTokenizersBackend\n",
    "    \n",
    "    # Check if patch is needed (if attribute is missing)\n",
    "    if not hasattr(OriginalTokenizersBackend, \"additional_special_tokens\"):\n",
    "        print(\"[INFO] Patching TokenizersBackend to handle missing 'additional_special_tokens'...\")\n",
    "        class PatchedTokenizersBackend(OriginalTokenizersBackend):\n",
    "            def __getattr__(self, key):\n",
    "                if key == \"additional_special_tokens\":\n",
    "                    return []\n",
    "                return super().__getattr__(key)\n",
    "        \n",
    "        import transformers.tokenization_utils_base\n",
    "        transformers.tokenization_utils_base.TokenizersBackend = PatchedTokenizersBackend\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# --- 3. LOAD MODEL ---\n",
    "print(\"[INFO] Loading model...\")\n",
    "embed_model = AutoModel.from_pretrained(\n",
    "    HF_EMBED_MODEL_ID,\n",
    "    config=config,\n",
    "    torch_dtype=model_dtype,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=attn_impl,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    ").eval()\n",
    "\n",
    "# Configure processor\n",
    "embed_model.processor.max_input_tiles = 6\n",
    "embed_model.processor.use_thumbnail = True\n",
    "\n",
    "def base64_to_pil(b64_str):\n",
    "    if not b64_str:\n",
    "        return None\n",
    "    try:\n",
    "        return Image.open(io.BytesIO(base64.b64decode(b64_str))).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- 4. VECTOR DB SETUP ---\n",
    "milvus_client = MilvusClient(MILVUS_URI)\n",
    "if milvus_client.has_collection(COLLECTION_NAME):\n",
    "    milvus_client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    dimension=2048,\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "# --- 5. INFERENCE LOOP ---\n",
    "vectors_to_insert = []\n",
    "print(\"[INFO] Generating Multimodal Embeddings...\")\n",
    "\n",
    "# Define safe limits based on hardware to prevent OOM on T4\n",
    "if use_flash_attn:\n",
    "    LIMIT_TEXT = 8192\n",
    "    LIMIT_MM = 10240\n",
    "else:\n",
    "    print(\"[INFO] T4/Eager mode active: Reducing embedding context window to 4096.\")\n",
    "    LIMIT_TEXT = 4096\n",
    "    LIMIT_MM = 4096\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for item in extracted_data:\n",
    "        metadata = item.get(\"metadata\", {})\n",
    "        content_meta = metadata.get(\"content_metadata\") or {}\n",
    "\n",
    "        doc_type = content_meta.get(\"type\", \"text\")\n",
    "        subtype = content_meta.get(\"subtype\", \"\")\n",
    "        content_text = metadata.get(\"content\", \"\")\n",
    "\n",
    "        image_obj = None\n",
    "        modality = \"text\"\n",
    "\n",
    "        # Check for Chart/Table images\n",
    "        b64_data = None\n",
    "        if doc_type == \"structured\" or subtype in [\"chart\", \"table\"]:\n",
    "            # Safe access to nested metadata\n",
    "            image_meta = metadata.get(\"image_metadata\") or {}\n",
    "            if image_meta.get(\"content\"):\n",
    "                 b64_data = image_meta[\"content\"]\n",
    "            elif len(content_text) > 100 and \" \" not in content_text[:50]:\n",
    "                 b64_data = content_text\n",
    "\n",
    "            if b64_data:\n",
    "                image_obj = base64_to_pil(b64_data)\n",
    "\n",
    "            if image_obj:\n",
    "                modality = \"image\"\n",
    "                table_meta = metadata.get(\"table_metadata\") or {}\n",
    "                table_content = table_meta.get(\"table_content\")\n",
    "                if table_content:\n",
    "                    content_text = table_content\n",
    "                    modality = \"image_text\"\n",
    "\n",
    "        if not content_text and not image_obj:\n",
    "            continue\n",
    "\n",
    "        # Encode\n",
    "        try:\n",
    "            if modality == \"image\":\n",
    "                embed_model.processor.p_max_length = 2048\n",
    "                embeddings = embed_model.encode_documents(images=[image_obj])\n",
    "            elif modality == \"image_text\":\n",
    "                embed_model.processor.p_max_length = LIMIT_MM\n",
    "                embeddings = embed_model.encode_documents(images=[image_obj], texts=[content_text])\n",
    "            else:\n",
    "                embed_model.processor.p_max_length = LIMIT_TEXT\n",
    "                embeddings = embed_model.encode_documents(texts=[content_text])\n",
    "\n",
    "            # L2 Normalize\n",
    "            def _l2_normalize(x: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "                return x / (x.norm(p=2, dim=-1, keepdim=True) + eps)\n",
    "\n",
    "            vec = _l2_normalize(embeddings[0]).float().cpu().numpy().tolist()\n",
    "\n",
    "            vectors_to_insert.append({\n",
    "                \"vector\": vec,\n",
    "                \"text\": content_text[:60000],\n",
    "                \"source\": metadata.get(\"source_metadata\", {}).get(\"source_name\", \"Unknown\"),\n",
    "                \"page\": content_meta.get(\"page_number\", -1),\n",
    "                \"type\": subtype or doc_type,\n",
    "                \"has_image\": True if image_obj else False,\n",
    "                \"image_b64\": b64_data if image_obj else \"\"\n",
    "            })\n",
    "\n",
    "            # Periodic cleanup for T4 fragmentation\n",
    "            if len(vectors_to_insert) % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"[WARN] Skipped embedding item due to OOM (Length: {len(content_text)}). Clearing cache.\")\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "milvus_client.insert(collection_name=COLLECTION_NAME, data=vectors_to_insert)\n",
    "print(f\"[SUCCESS] Indexed {len(vectors_to_insert)} items.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdcdec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HARDWARE] Detected: NVIDIA L40S | VRAM: 44.4GB | Compute: 8.9\n",
      "[CONFIG] Mode: High Performance (Flash Attention 2)\n",
      "[STRATEGY] High VRAM detected. Keeping models resident in memory.\n",
      "[INFO] Universal Retrieval Pipeline Ready (Strategy: flash_attention_2).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForSequenceClassification, AutoProcessor, AutoConfig, AutoModel\n",
    "\n",
    "# Configuration\n",
    "HF_EMBED_MODEL_ID = \"nvidia/llama-nemotron-embed-vl-1b-v2\"\n",
    "HF_RERANK_MODEL_ID = \"nvidia/llama-nemotron-rerank-vl-1b-v2\"\n",
    "\n",
    "# --- SMART HARDWARE DETECTION ---\n",
    "def get_hardware_strategy():\n",
    "    \"\"\"\n",
    "    Determines precision, attention, tiling, and memory strategy.\n",
    "    Checks BOTH hardware capability AND library availability.\n",
    "    \"\"\"\n",
    "    strategy = {\n",
    "        \"dtype\": torch.float32,\n",
    "        \"attn\": \"eager\",\n",
    "        \"tiles\": 1,\n",
    "        \"swap_models\": True # Default to safe mode (swap)\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        major, minor = props.major, props.minor\n",
    "        vram_gb = props.total_memory / (1024**3)\n",
    "        \n",
    "        print(f\"[HARDWARE] Detected: {props.name} | VRAM: {vram_gb:.1f}GB | Compute: {major}.{minor}\")\n",
    "        \n",
    "        # Check if Flash Attention library is actually installed\n",
    "        has_flash_lib = False\n",
    "        try:\n",
    "            import flash_attn\n",
    "            has_flash_lib = True\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "        # 1. Architecture Strategy\n",
    "        if major >= 8: # Ampere/Hopper (A100, H100)\n",
    "            strategy[\"dtype\"] = torch.bfloat16\n",
    "            \n",
    "            if has_flash_lib:\n",
    "                strategy[\"attn\"] = \"flash_attention_2\"\n",
    "                strategy[\"tiles\"] = 6 # Max resolution\n",
    "                print(\"[CONFIG] Mode: High Performance (Flash Attention 2)\")\n",
    "            else:\n",
    "                strategy[\"attn\"] = \"eager\"\n",
    "                strategy[\"tiles\"] = 6 # H100 has 80GB, can handle high-res eager\n",
    "                print(\"[CONFIG] Mode: Fallback (Eager Attention on High-End GPU)\")\n",
    "                \n",
    "        else: # Turing (T4)\n",
    "            strategy[\"dtype\"] = torch.float16\n",
    "            strategy[\"attn\"] = \"eager\"\n",
    "            strategy[\"tiles\"] = 1 # Low resolution for memory safety\n",
    "            print(\"[CONFIG] Mode: Low VRAM (T4 Optimization)\")\n",
    "            \n",
    "        # 2. Memory Strategy (Swap vs Keep)\n",
    "        # If we have > 24GB VRAM, we can keep both models loaded safely.\n",
    "        if vram_gb > 24.0:\n",
    "            strategy[\"swap_models\"] = False\n",
    "            print(\"[STRATEGY] High VRAM detected. Keeping models resident in memory.\")\n",
    "        else:\n",
    "            strategy[\"swap_models\"] = True\n",
    "            print(\"[STRATEGY] Low VRAM detected. Enabling Aggressive Model Swapping.\")\n",
    "            \n",
    "    return strategy\n",
    "\n",
    "config_strat = get_hardware_strategy()\n",
    "\n",
    "# Global placeholders\n",
    "embed_model = None\n",
    "rerank_model = None\n",
    "rerank_processor = None\n",
    "\n",
    "def apply_safe_config(config_obj):\n",
    "    \"\"\"\n",
    "    Prevents the model from auto-enabling Flash Attention \n",
    "    when the library is installed but broken (missing nvcc) OR when on T4.\n",
    "    \"\"\"\n",
    "    # If we decided on Eager, force the config to match\n",
    "    if config_strat[\"attn\"] == \"eager\":\n",
    "        if hasattr(config_obj, \"llm_config\"):\n",
    "            original_class = config_obj.llm_config.__class__\n",
    "            \n",
    "            # Create a \"Safe\" config class that ignores attempts to set FA2\n",
    "            class SafeConfig(original_class):\n",
    "                def __setattr__(self, name, value):\n",
    "                    # Block attempts to enable FA2\n",
    "                    if name == \"_attn_implementation\" and value == \"flash_attention_2\":\n",
    "                        return \n",
    "                    super().__setattr__(name, value)\n",
    "            \n",
    "            # Apply the safe class to the config object\n",
    "            config_obj.llm_config.__class__ = SafeConfig\n",
    "            config_obj.llm_config._attn_implementation = \"eager\"\n",
    "            \n",
    "    return config_obj\n",
    "\n",
    "def load_embed_model():\n",
    "    global embed_model\n",
    "    if embed_model is not None: return\n",
    "    \n",
    "    print(\"[LOAD] Loading Embedding Model...\")\n",
    "    cfg = AutoConfig.from_pretrained(HF_EMBED_MODEL_ID, trust_remote_code=True)\n",
    "    cfg = apply_safe_config(cfg) \n",
    "    \n",
    "    embed_model = AutoModel.from_pretrained(\n",
    "        HF_EMBED_MODEL_ID,\n",
    "        config=cfg,\n",
    "        torch_dtype=config_strat[\"dtype\"],\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=config_strat[\"attn\"],\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "\n",
    "def load_rerank_model():\n",
    "    global rerank_model, rerank_processor\n",
    "    if rerank_model is not None: return\n",
    "\n",
    "    print(f\"[LOAD] Loading Reranker (Tiles={config_strat['tiles']})...\")\n",
    "    cfg = AutoConfig.from_pretrained(HF_RERANK_MODEL_ID, trust_remote_code=True)\n",
    "    cfg = apply_safe_config(cfg)\n",
    "    \n",
    "    rerank_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        HF_RERANK_MODEL_ID,\n",
    "        config=cfg,\n",
    "        torch_dtype=config_strat[\"dtype\"],\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=config_strat[\"attn\"],\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "    \n",
    "    rerank_processor = AutoProcessor.from_pretrained(\n",
    "        HF_RERANK_MODEL_ID, \n",
    "        trust_remote_code=True,\n",
    "        max_input_tiles=config_strat[\"tiles\"],\n",
    "        use_thumbnail=True\n",
    "    )\n",
    "\n",
    "def unload_model(model_ref):\n",
    "    \"\"\"Forcefully removes a model from VRAM.\"\"\"\n",
    "    if model_ref is None: return None\n",
    "    del model_ref\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return None\n",
    "\n",
    "def retrieve_and_rerank(query, retrieve_k=20, final_k=5):\n",
    "    global embed_model, rerank_model, rerank_processor\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # STAGE 1: DENSE RETRIEVAL\n",
    "    # ---------------------------------------------------------\n",
    "    load_embed_model()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_emb = embed_model.encode_queries([query])[0].float().cpu().numpy().tolist()\n",
    "    \n",
    "    # If low VRAM, unload embedding model immediately\n",
    "    if config_strat[\"swap_models\"]:\n",
    "        embed_model = unload_model(embed_model)\n",
    "        \n",
    "    hits = milvus_client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        data=[q_emb],\n",
    "        limit=retrieve_k,\n",
    "        output_fields=[\"text\", \"page\", \"source\", \"type\", \"has_image\", \"image_b64\"]\n",
    "    )[0]\n",
    "    \n",
    "    if not hits:\n",
    "        return [], []\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # STAGE 2: RERANKING\n",
    "    # ---------------------------------------------------------\n",
    "    load_rerank_model()\n",
    "    \n",
    "    rerank_inputs = []\n",
    "    valid_hits = []\n",
    "    \n",
    "    for hit in hits:\n",
    "        ent = hit['entity']\n",
    "        img_obj = base64_to_pil(ent.get(\"image_b64\")) if ent.get(\"has_image\") else None\n",
    "        # Truncate text if needed\n",
    "        txt_limit = 1000 if config_strat[\"swap_models\"] else 8000\n",
    "        text_content = ent['text'][:txt_limit] if ent['text'] else \"\"\n",
    "        \n",
    "        rerank_inputs.append({\n",
    "            \"question\": query,\n",
    "            \"doc_text\": text_content,\n",
    "            \"doc_image\": img_obj if img_obj else \"\"\n",
    "        })\n",
    "        valid_hits.append(hit)\n",
    "\n",
    "    # Process\n",
    "    all_scores = []\n",
    "    # Batch size: 1 for T4 safety, 4 for H100 speed\n",
    "    batch_size = 4 if not config_strat[\"swap_models\"] else 1\n",
    "    \n",
    "    for i in range(0, len(rerank_inputs), batch_size):\n",
    "        batch = rerank_inputs[i:i+batch_size]\n",
    "        inputs = rerank_processor.process_queries_documents_crossencoder(batch)\n",
    "        inputs = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = rerank_model(**inputs)\n",
    "            logits = outputs.logits.squeeze(-1).float().cpu().numpy()\n",
    "            if logits.ndim == 0: all_scores.append(float(logits))\n",
    "            else: all_scores.extend(logits.tolist())\n",
    "            \n",
    "        del inputs, outputs\n",
    "        if config_strat[\"swap_models\"] and i % 5 == 0: \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # If low VRAM, unload reranker\n",
    "    if config_strat[\"swap_models\"]:\n",
    "        rerank_model = unload_model(rerank_model)\n",
    "        rerank_processor = unload_model(rerank_processor)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # SORT & RETURN\n",
    "    # ---------------------------------------------------------\n",
    "    for i, score in enumerate(all_scores):\n",
    "        valid_hits[i]['score'] = score\n",
    "    \n",
    "    valid_hits.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return hits, valid_hits[:final_k]\n",
    "\n",
    "print(f\"[INFO] Universal Retrieval Pipeline Ready (Strategy: {config_strat['attn']}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf928eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def generate_enterprise_answer(query, hits):\n",
    "    context_str = \"\"\n",
    "    for i, hit in enumerate(hits):\n",
    "        ent = hit['entity']\n",
    "        type_label = ent['type'].upper()\n",
    "        context_str += f\"[Source ID: {i+1} | Type: {type_label} | Page: {ent['page']}]\\n{ent['text']}\\n\\n\"\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an expert Document Intelligence Analyst. Your goal is to extract precise insights from complex multimodal documents (text, charts, and tables).\\n\"\n",
    "        \"Answer based ONLY on the provided context chunks.\\n\\n\"\n",
    "        \"GUIDELINES:\\n\"\n",
    "        \"1. **Start with a <think> block**: Before answering, explicitly reason through the following checks:\\n\"\n",
    "        \"   - **Visual Legend Check**: If analyzing a chart, identify the legend or color coding first. Ensure you are extracting data for the specific entity requested (e.g., the specific country/company vs. the benchmark/competitor).\\n\"\n",
    "        \"   - **Table Logic**: If extracting data from a table, look for 'Total', 'All', or 'Average' rows/columns before defaulting to granular sub-categories (like 'Small/Medium/Large').\\n\"\n",
    "        \"   - **Numerical Precision**: Be vigilant for decimal points and units (e.g., '0.5' vs '5', 'millions' vs 'billions'). If a number seems like an OCR error, verify it against surrounding text.\\n\"\n",
    "        \"   - **Temporal Filtering**: Ensure the data corresponds to the specific time period requested in the user's query.\\n\"\n",
    "        \"2. **Citation**: Every factual statement must cite its source exactly as provided (e.g., '[Source ID: 1, Page: 5]').\\n\"\n",
    "        \"3. **Honesty**: If the exact data point is not in the retrieval, state 'The provided context does not contain this specific data' rather than hallucinating.\\n\"\n",
    "        \"4. **Format**: After your <think> analysis, provide the final response starting with 'Answer:'.\"\n",
    "    )\n",
    "\n",
    "    client_llm = OpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=os.environ[\"NVIDIA_API_KEY\"]\n",
    "    )\n",
    "\n",
    "    response = client_llm.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context_str}\\n\\nQuestion: {query}\"}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def display_formatted_response(question, raw_output):\n",
    "    \"\"\"Display response with collapsible reasoning.\"\"\"\n",
    "    safe_text = raw_output.replace('$', '&dollar;')\n",
    "    thought_match = re.search(r'<think>(.*?)</think>', safe_text, re.DOTALL)\n",
    "\n",
    "    if thought_match:\n",
    "        reasoning = thought_match.group(1).strip()\n",
    "        answer = safe_text.split('</think>')[-1].strip()\n",
    "    else:\n",
    "        reasoning = \"No internal reasoning trace provided.\"\n",
    "        answer = safe_text\n",
    "\n",
    "    output_md = f\"\"\"\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "<details>\n",
    "<summary><strong>View Reasoning Process</strong></summary>\n",
    "<br>\n",
    "<div style=\"padding-left: 15px; border-left: 3px solid #ddd; color: #555;\">\n",
    "\n",
    "{reasoning}\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "### Answer\n",
    "{answer}\n",
    "<hr style=\"margin-top: 20px; border: 0; border-top: 1px solid #eee;\">\n",
    "\"\"\"\n",
    "    display(Markdown(output_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b23703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Enterprise Intelligent Document Processing: LIVE DEMO\n",
      "============================================================\n",
      "[LOAD] Loading Embedding Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_image_token, max_input_tiles, pad_to_multiple_of, dynamic_image_size, p_max_length, use_thumbnail, passage_prefix, image_size, norm_type, template, query_prefix, num_channels, padding, system_message, q_max_length. \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] Loading Reranker (Tiles=6)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0f2111d8ff47f18cf00e3d1a325939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fae90d8cd5b474e9a68e092b8ae5a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_llama_nemotron_vl.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-rerank-vl-1b-v2:\n",
      "- configuration_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59438f49102b40fba8e21ee557255602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_llama_nemotron_vl.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c90442ebe64dd4865d9a5c075f5cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_llama_nemotron_vl.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-rerank-vl-1b-v2:\n",
      "- processing_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/nvidia/llama-nemotron-rerank-vl-1b-v2:\n",
      "- modeling_llama_nemotron_vl.py\n",
      "- processing_llama_nemotron_vl.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c620a892e334f3abdd558c52edf31d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93db5e47e6f45ad8eeb16fe5d619d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7890d83624846b7ae01be1071e14bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f87a0dfc4447f78c17e62072b9ab26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005854048887440c95d402dd8b458168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: per_tile_len, num_image_token, max_input_tiles, pad_to_multiple_of, dynamic_image_size, rerank_max_length, use_thumbnail, template, padding, prompt_template. \n",
      "Some kwargs in processor config are unused and will not have any effect: per_tile_len, num_image_token, max_input_tiles, pad_to_multiple_of, dynamic_image_size, rerank_max_length, use_thumbnail, template, padding, prompt_template. \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Question\n",
       "การยกเลิกเอกสารรายละเอียดผู้ขายต้องทำยังไงบ้าง\n",
       "\n",
       "<details>\n",
       "<summary><strong>View Reasoning Process</strong></summary>\n",
       "<br>\n",
       "<div style=\"padding-left: 15px; border-left: 3px solid #ddd; color: #555;\">\n",
       "\n",
       "Okay, let's tackle this question about how to cancel a supplier document in the Q-ERP system. The user is asking for the steps to cancel a supplier document. I need to refer to the provided context chunks to find the exact procedure.\n",
       "\n",
       "First, I'll look through the context for sections related to canceling supplier documents. From the context, there's a section under \"โปรแกรมผู้ขาย\" (Supplier Program) that talks about canceling documents. \n",
       "\n",
       "Looking at Source ID: 1, Page: 27, there's a part that says: \"3. การยกเลิกเอกสารรายละเอียดผู้ขาย 3.1 ในกรณีทีÉผู้ใช้ต้องการยกเลิกสถานะใบสัÉงซืÊอบางรายการ สามารถทําได้โดยเลือกข้อมูลของผู้ขาย รายทีÉต้องการยกเลิกสถานะ แล้วกดปุ ่ ม หรือเลือกคลิกทีÉแถบเมนูทีÉเมนูสถานะข้อมูล ยกเลิก ดังรูปทีÉ PO-44 เมืÉอผู้ใช้เลือก สถานะยกเลิก ให้แก่ใบสัÉงซืÊอแล้ว สีทีÉแสดงข้อมูลของ ใบสัÉงซืÊอจะเปลีÉยนเป็นสีแดง\"\n",
       "\n",
       "Wait, but this seems to be about canceling a purchase order (ใบสัÉงซืÊอ), not the supplier document itself. Let me check again.\n",
       "\n",
       "Further down in the same source, under \"2. การแก้ไขเอกสารรายละเอียดผู้ขาย\", there's a mention of canceling. Wait, no. Let me check the section specifically about canceling supplier documents.\n",
       "\n",
       "Looking back, in the \"โปรแกรมผู้ขาย\" section, under \"2.1.5 เครืÉองมือสําหรับสถานะ : ยกเลิก (Status : Cancel Tool) ใช้สําหรับยกเลิกสถานะของผู้ขาย เพืÉอยกเลิกรายการนัÊน ปรากฏดังรูปทีÉ 12\" (Source ID: 1, Page: 12). But the steps for cancellation might be in the later sections.\n",
       "\n",
       "In the same source, under \"3. การยกเลิกเอกสารรายละเอียดผู้ขาย\" (Section 3: Cancellation of Supplier Document Details), the steps are:\n",
       "\n",
       "3.1 Select the supplier data to be canceled from the list.\n",
       "3.2 Click the tool button or select the status menu → Cancel.\n",
       "3.3 The system will confirm, and the status of the supplier will change to canceled, displayed in red.\n",
       "\n",
       "So the steps are: select the supplier record, use the cancel tool or status menu to cancel, confirm, and the status changes to red.\n",
       "\n",
       "I need to make sure there are no other steps mentioned. The context mentions that after cancellation, the status is shown in red, and the user can still view the data but cannot edit or use it again.\n",
       "\n",
       "Therefore, the answer should outline these steps based on the provided context.\n",
       "\n",
       "</div>\n",
       "</details>\n",
       "\n",
       "### Answer\n",
       "Answer: การยกเลิกเอกสารรายละเอียดผู้ขายในระบบ Q-ERP สามารถทำได้ตามขั้นตอนดังนี้ [Source ID: 1, Page: 27]:  \n",
       "1. เลือกข้อมูลผู้ขายที่ต้องการยกเลิกสถานะจากตารางข้อมูล  \n",
       "2. คลิกปุ่ม **สถานะ → ยกเลิก** (Status: Cancel Tool) บนส่วนกล่องเครื่องมือ หรือเลือกเมนูสถานะข้อมูล → ยกเลิก  \n",
       "3. ระบบจะขอการยืนยันการยกเลิก  \n",
       "4. หลังยืนยัน สีที่แสดงข้อมูลผู้ขายในตารางจะเปลี่ยนเป็นสีแดง หมายถึงสถานะถูกยกเลิก  \n",
       "\n",
       "**หมายเหตุ:** ข้อมูลผู้ขายที่ถูกยกเลิกยังสามารถดูรายละเอียดได้ แต่ไม่สามารถแก้ไขหรือนำไปใช้งานในระบบได้อีกต่อไป [Source ID: 1, Page: 27].\n",
       "<hr style=\"margin-top: 20px; border: 0; border-top: 1px solid #eee;\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Question\n",
       "การสร้างเอกสารรายละเอียดใบสั่งซื้อต้องทำยังไงบ้าง\n",
       "\n",
       "<details>\n",
       "<summary><strong>View Reasoning Process</strong></summary>\n",
       "<br>\n",
       "<div style=\"padding-left: 15px; border-left: 3px solid #ddd; color: #555;\">\n",
       "\n",
       "Okay, let's tackle this question about creating a purchase order document in the Q-ERP system. The user is asking for the steps to create a purchase order. I need to refer to the provided context chunks to find the relevant information.\n",
       "\n",
       "First, I'll look through the context for sections related to creating a purchase order. From the context, there's a section titled \"การสร้างเอกสารรายละเอียดใบสัÉงซืÊอ\" which translates to creating a purchase order document. \n",
       "\n",
       "Looking at the steps mentioned in the context, the process starts by accessing the Purchase menu and selecting the Purchase Order document form. Then, the user needs to click the 'New' tool in the toolbar to start creating a new document. The system will display a form with various fields that need to be filled in. Some fields are mandatory, marked with an asterisk (*), so those must be completed. \n",
       "\n",
       "The user might need to select data from existing records, like customer or product information, using the search tools provided. For example, clicking on certain fields might open a search window to select a customer or product from the database. After filling in all necessary details, the user saves the document by clicking the 'Save' button. There might be a confirmation prompt to ensure the user wants to proceed with saving.\n",
       "\n",
       "I should also check if there are any specific details mentioned, like required fields or additional steps for different types of purchases (domestic vs. international). The context mentions that the system handles both domestic and international purchases, so the user might need to specify that. \n",
       "\n",
       "Additionally, there's a mention of a 'Print Preview' option, so after saving, the user might have the option to print the document. However, the question is specifically about creating the document, so printing might be an optional step.\n",
       "\n",
       "I need to make sure the steps are in order and reference the correct source IDs where the information is found. The context chunks from Source ID: 1 (the main text) and possibly others like Source ID: 6 or 10 which have tables related to purchase orders might be relevant. \n",
       "\n",
       "Wait, looking at the context, the detailed steps for creating a purchase order are in the section under \"โปรแกรมเอกสารใบสัÉงซืÊอ\" (Purchase Order Document Program). The steps are outlined from 1.1 to 1.4, including accessing the menu, using the toolbar, filling in the form, and saving. There's also mention of using search tools for selecting data, which is part of the process.\n",
       "\n",
       "I should structure the answer by listing the steps in order, citing the source where each step is described. Also, note any mandatory fields and the use of search functions. Since the user might be looking for a concise step-by-step guide, I'll present it clearly, ensuring each step references the correct part of the context.\n",
       "\n",
       "</div>\n",
       "</details>\n",
       "\n",
       "### Answer\n",
       "Answer: การสร้างเอกสารรายละเอียดใบสั่งซื้อ (Purchase Order) ในระบบ Q-ERP มีขั้นตอนดังนี้ [Source ID: 1, Page: 67-74]:  \n",
       "\n",
       "1. **เข้าเมนู ระบบซื้อ (Purchase)**  \n",
       "   - คลิกเมนู **ใบสั่งซื้อ (Purchase Order)** ในหน้าจอ Q-ERP Menu  \n",
       "\n",
       "2. **ใช้เครื่องมือสร้างเอกสารใหม่**  \n",
       "   - คลิกปุ่ม **สร้าง (New)** ในกล่องเครื่องมือ เพื่อเริ่มสร้างเอกสารใบสั่งซื้อ  \n",
       "\n",
       "3. **กรอกข้อมูลหลัก**  \n",
       "   - กรอกข้อมูลที่มีเครื่องหมาย * (필수) เช่น  \n",
       "     - **เลขที่เอกสาร (Document Number)**  \n",
       "     - **วันที่สั่งซื้อ (Order Date)**  \n",
       "     - **สกุลเงิน (Currency)**  \n",
       "     - **รหัสผู้ขาย (Supplier Code)**  \n",
       "     - **ชื่อผู้ขาย (Supplier Name)**  \n",
       "   - ใช้ปุ่มค้นหา (ปุ่มที่มีไอคอนลูกศรหรือเครื่องหมาย...) เพื่อเลือกข้อมูลจากฐานข้อมูล (เช่น ผู้ขาย, สินค้า)  \n",
       "\n",
       "4. **เพิ่มรายการสินค้า/บริการ**  \n",
       "   - ในส่วน **รายการ (Items)**:  \n",
       "     - คลิกที่ช่องว่างในตารางเพื่อเพิ่มรายการ  \n",
       "     - ค้นหาสินค้าโดยคลิกปุ่มค้นหา (ไอคอนลูกศร) ที่ช่อง **รหัสสินค้า (Item Code)**  \n",
       "     - กรอก **จํานวน (Quantity)**, **หน่วย (Unit)**, **ราคา/หน่วย (Unit Price)**  \n",
       "     - ระบบจะคำนวณ **รวมเงิน (Total)** อัตโนมัติ  \n",
       "\n",
       "5. **บันทึกเอกสาร**  \n",
       "   - คลิกปุ่ม **บันทึก (Save)** ในกล่องเครื่องมือ  \n",
       "   - ระบบจะขอ確認การบันทึก -> คลิก **ใช่ (Yes)**  \n",
       "\n",
       "6. **พิมพ์เอกสาร (ถ้าต้องการ)**  \n",
       "   - คลิกปุ่ม **พิมพ์ (Print)** หรือ **ดู قبلพิมพ์ (Print Preview)** ในกล่องเครื่องมือ  \n",
       "\n",
       "**หมายเหตุ**:  \n",
       "- ข้อมูลที่มีเครื่องหมาย * ต้องกรอกให้ครบ  \n",
       "- สามารถแก้ไขข้อมูลได้โดยการคลิกปุ่ม **แก้ไข (Edit)** หลังบันทึกครั้งแรก [Source ID: 1, Page: 74]  \n",
       "\n",
       "[Source ID: 1, Page: 67-74]\n",
       "<hr style=\"margin-top: 20px; border: 0; border-top: 1px solid #eee;\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Question\n",
       "วิธียกเลิกข้อมูลใบขอจัดซื้อต้องทำยังไงบ้าง\n",
       "\n",
       "<details>\n",
       "<summary><strong>View Reasoning Process</strong></summary>\n",
       "<br>\n",
       "<div style=\"padding-left: 15px; border-left: 3px solid #ddd; color: #555;\">\n",
       "\n",
       "Okay, let's tackle the user's question: \"วิธียกเลิกข้อมูลใบขอจัดซื้อต้องทำยังไงบ้าง\" which translates to \"How to cancel purchase request data?\" \n",
       "\n",
       "First, I need to refer to the provided context chunks to find the relevant information. The user is asking about the process to cancel a purchase request in the Q-ERP system.\n",
       "\n",
       "Looking through the context, I remember that there's a section about the Purchase Requisition program. Let me check the sources. \n",
       "\n",
       "From Source ID: 1, there's a section titled \"โปรแกรมเอกสารใบขอซื้อ\" (Purchase Requisition Program). Specifically, under the part about canceling data, there's a step-by-step guide. \n",
       "\n",
       "In the context, under the section for canceling purchase request data, the steps are outlined. The user needs to select the data they want to cancel, then click the cancel status button in the tool menu. Then confirm the cancellation. \n",
       "\n",
       "Also, there's a mention that canceled data will be displayed in red and can still be viewed but not edited or used again. \n",
       "\n",
       "I should make sure there are no other steps or details in other sources. Checking other tables or text chunks, but it seems the main info is in Source ID: 1. \n",
       "\n",
       "So the answer should outline the steps: selecting the data, using the cancel tool, confirming, and noting the red text indication. Also, mention that it can't be edited after cancellation.\n",
       "\n",
       "</div>\n",
       "</details>\n",
       "\n",
       "### Answer\n",
       "Answer: การยกเลิกข้อมูลใบขอจัดซื้อ (Purchase Requisition) ในระบบ Q-ERP ทำได้โดยเลือกข้อมูลใบขอจัดซื้อที่ต้องการยกเลิกจากตารางแสดงรายการ แล้วคลิกปุ่ม **\"ยกเลิกสถานะ\"** ในกล่องเครื่องมือ (หรือเลือกจากแถบเมนู **\"สถานะข้อมูล → ยกเลิก\"**) หลังจากนั้น ระบบจะขอการยืนยันการยกเลิก โดยคลิก **\"YES\"** เพื่อเสร็จสิ้นการยกเลิก [Source ID: 1, Page: 51]. \n",
       "\n",
       "**หมายเหตุ:** ข้อมูลที่ถูกยกเลิกแล้วจะแสดงสถานะเป็นตัวอักษรสีแดงในตาราง และยังสามารถดูรายละเอียดได้ แต่ไม่สามารถแก้ไขหรือนำข้อมูลนั้นมาใช้งานได้อีก [Source ID: 1, Page: 51].\n",
       "<hr style=\"margin-top: 20px; border: 0; border-top: 1px solid #eee;\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Question\n",
       "การยกเลิกสถานะใบสั่งซื้อต้องทำยังไงบ้าง\n",
       "\n",
       "<details>\n",
       "<summary><strong>View Reasoning Process</strong></summary>\n",
       "<br>\n",
       "<div style=\"padding-left: 15px; border-left: 3px solid #ddd; color: #555;\">\n",
       "\n",
       "Okay, let's tackle this question about how to cancel the status of a purchase order in the Q-ERP system. The user is asking for the steps involved in this process.\n",
       "\n",
       "First, I need to recall the information provided in the context. From the documents, especially the sections related to the Purchase Order (ใบสั่งซื้อ) program, there's a mention of a \"Status: Cancel Tool\" in the toolbar. The context also describes that selecting a purchase order and then clicking on the status cancel option will change the status to canceled, and the text color turns red to indicate cancellation.\n",
       "\n",
       "Looking at the specific context chunks, in Source ID: 1, under the section about the Purchase Order program, there's a part that says: \"หากสังเกตให้ดี เมื่อผู้ใช้เลือก สถานะยกเลิก ให้แก่ใบสั่งซื้อแล้ว สีที่แสดงข้อมูลของใบสั่งซื้อจะเปลี่ยนเป็นสีแดง\" which translates to the status changing to red when canceled. Also, in the same section, there's a step-by-step guide on how to cancel, mentioning selecting the order and using the status cancel tool.\n",
       "\n",
       "Additionally, in Source ID: 6 (Page: 58), there's a table showing some data with a column labeled \"Un\" which might indicate the status, but it's not entirely clear. However, the main instructions are in the text sections.\n",
       "\n",
       "So, putting it all together, the steps would involve:\n",
       "\n",
       "1. Accessing the Purchase Order module.\n",
       "2. Selecting the specific purchase order to cancel.\n",
       "3. Using the \"Status: Cancel Tool\" from the toolbar or menu to change the status.\n",
       "4. Confirming the cancellation, which changes the order's status and displays it in red.\n",
       "\n",
       "I need to make sure there are no other steps mentioned, like needing to enter a reason for cancellation or any approvals required. The context doesn't mention any additional steps beyond selecting the order and using the cancel tool. Also, the cancellation seems to be a straightforward action without requiring further input beyond confirmation.\n",
       "\n",
       "Another thing to check is if there are any prerequisites, like the order not being partially fulfilled or having related invoices. The context doesn't specify such conditions, so it's assumed that the user can cancel any order as long as it's in a cancellable state.\n",
       "\n",
       "Therefore, the answer should outline the steps based on the provided context, citing the relevant source IDs where the information was found.\n",
       "\n",
       "</div>\n",
       "</details>\n",
       "\n",
       "### Answer\n",
       "Answer: การยกเลิกสถานะใบสั่งซื้อ (Purchase Order) ในระบบ Q-ERP สามารถทำได้ตามขั้นตอนดังนี้:\n",
       "\n",
       "1. **เลือกใบสั่งซื้อที่ต้องการยกเลิก**  \n",
       "   - คลิกเลือกข้อมูลใบสั่งซื้อที่ต้องการยกเลิกในตารางแสดงรายการ (ดังรูปทีÉ PO-44)\n",
       "\n",
       "2. **ใช้เครื่องมือยกเลิกสถานะ**  \n",
       "   - คลิกปุ่ม **\"สถานะ: ยกเลิก\"** (Status: Cancel Tool) บนส่วนกล่องเครื่องมือ หรือเลือกจากแถบเมนู **\"สถานะข้อมูล → ยกเลิก\"**  \n",
       "   - ระบบจะแสดงหน้าต่างยืนยันการยกเลิก\n",
       "\n",
       "3. **ยืนยันการยกเลิก**  \n",
       "   - คลิกปุ่ม **\"YES\"** เพื่อยืนยันการยกเลิกสถานะใบสั่งซื้อ  \n",
       "   - หลังการยกเลิก สีที่แสดงข้อมูลใบสั่งซื้อจะเปลี่ยนเป็นสีแดง (ดังรูปทีÉ PO-45)\n",
       "\n",
       "**แหล่งข้อมูลอ้างอิง:**  \n",
       "- [Source ID: 1, Page: 75] (ส่วนการยกเลิกสถานะใบสั่งซื้อ)  \n",
       "- [Source ID: 1, Page: 56] (ส่วนกล่องเครื่องมือของโปรแกรมใบสั่งซื้อ)  \n",
       "\n",
       "**หมายเหตุ:**  \n",
       "- การยกเลิกสถานะใบสั่งซื้อจะไม่เปลี่ยนแปลงข้อมูลรายละเอียด แต่จะแสดงสถานะว่าเป็น \"ยกเลิก\" และสีแดงในตาราง  \n",
       "- ผู้ใช้สามารถดูรายละเอียดของใบสั่งซื้อที่ถูกยกเลิกได้ แต่ไม่สามารถแก้ไขหรือนำข้อมูลไปใช้ในการทำงานต่อได้\n",
       "<hr style=\"margin-top: 20px; border: 0; border-top: 1px solid #eee;\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    # 1. Reading bar charts for growth stats\n",
    "    \"การยกเลิกเอกสารรายละเอียดผู้ขายต้องทำยังไงบ้าง\",\n",
    "\n",
    "    # 2. Reading charts regarding infrastructure reliability\n",
    "    \"การสร้างเอกสารรายละเอียดใบสั่งซื้อต้องทำยังไงบ้าง\",\n",
    "\n",
    "    # 3. Table Query: Column/Row lookup\n",
    "    \"วิธียกเลิกข้อมูลใบขอจัดซื้อต้องทำยังไงบ้าง\",\n",
    "\n",
    "    # 4. Table Query: Comparative analysis\n",
    "    \"การยกเลิกสถานะใบสั่งซื้อต้องทำยังไงบ้าง\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Enterprise Intelligent Document Processing: LIVE DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in questions:\n",
    "    # 1. Retrieve & Rerank\n",
    "    # We only want the final reranked results for the generation step\n",
    "    _, hits = retrieve_and_rerank(q, retrieve_k=50, final_k=20)\n",
    "\n",
    "    # 2. Generate\n",
    "    if hits:\n",
    "        # Generate raw text from the LLM\n",
    "        raw_answer = generate_enterprise_answer(q, hits)\n",
    "\n",
    "        # Format and display cleanly\n",
    "        display_formatted_response(q, raw_answer)\n",
    "\n",
    "    else:\n",
    "        print(f\"[No relevant data found for query: {q}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemotron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
